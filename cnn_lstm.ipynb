{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CNN-LSTM Hybrid Model\n",
        "We use a CNN-LSTM hybrid architecture, where 1D Convolutional Neural Networks (CNNs) are employed for feature extraction from ECG signals, and Long Short-Term Memory (LSTM) layers are used to capture temporal dependencies across time steps."
      ],
      "metadata": {
        "id": "rdCujjIor1tB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-yt1xvjmYts"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNNLSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_channels=1, num_classes=5):\n",
        "        super(CNNLSTMClassifier, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=64, kernel_size=50)\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=20, stride=2)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=10)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=10, stride=2)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=5)\n",
        "        self.pool3 = nn.MaxPool1d(kernel_size=5, stride=2)\n",
        "        self.dropout3 = nn.Dropout(0.1)\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=16, hidden_size=32, batch_first=True)\n",
        "\n",
        "        self.fc1 = nn.Linear(32, 32)\n",
        "        self.dropout4 = nn.Dropout(0.1)\n",
        "        self.fc2 = nn.Linear(32, 16)\n",
        "        self.fc3 = nn.Linear(16, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, time_steps, input_channels)\n",
        "        x = x.permute(0, 2, 1)  # Convert to (batch, channels, time)\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.pool3(x)\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        x = x.permute(0, 2, 1)  # Convert to (batch, time_steps, features) for LSTM\n",
        "        self.lstm.flatten_parameters()\n",
        "        x, _ = self.lstm(x)\n",
        "\n",
        "        x = x[:, -1, :]  # Use the last LSTM output\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout4(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)  # Final output\n",
        "        return F.log_softmax(x, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "\n",
        "# ----- Dataset Class -----\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)  # Categorical: use class indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx].unsqueeze(-1)  # Shape: (500, 1)\n",
        "        y = self.y[idx]\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "fwoh3MzcnGtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After preprocessing the ECG signals, the segmented data (X_segments.npy) and corresponding labels (y_labels.npy) are saved and uploaded to Google Drive for use within this Colab notebook."
      ],
      "metadata": {
        "id": "1gsFhVRFsekl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "X_path = '/content/drive/MyDrive/X_segments.npy'\n",
        "y_path = '/content/drive/MyDrive/y_labels.npy'\n",
        "\n",
        "X = np.load(X_path)\n",
        "y = np.load(y_path)\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YajYIgE8ZY_0",
        "outputId": "13e8241b-5f27-44f8-a893-921c88cf8330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "X shape: (110658, 500)\n",
            "y shape: (110658,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All original beat annotations are mapped to the AAMI-recommended five-class system. Each AAMI class corresponds to the following:\n",
        "\n",
        "1. N (Normal): Includes normal beats and bundle branch blocks.\n",
        "2. S (Supraventricular ectopic): Includes atrial premature beats, aberrant atrial rhythms.\n",
        "3. V (Ventricular ectopic): Includes premature ventricular contractions, ventricular escape beats.\n",
        "4. F (Fusion): Includes fusion of ventricular and normal beats.\n",
        "5. Q (Unknown): Includes paced beats, unclassifiable beats, and artifacts.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0xpp4zMFtC01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# AAMI class mapping\n",
        "aami_map = {\n",
        "    'N': 'N', 'L': 'N', 'R': 'N', 'e': 'N', 'j': 'N',\n",
        "    'A': 'S', 'a': 'S', 'J': 'S', 'S': 'S',\n",
        "    'V': 'V', 'E': 'V',\n",
        "    'F': 'F'\n",
        "    # Everything else goes to 'Q'\n",
        "}\n",
        "\n",
        "# Apply mapping; default to 'Q'\n",
        "y_mapped = np.array([aami_map.get(label, 'Q') for label in y])\n",
        "\n",
        "# Encode class labels to integers\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y_mapped)\n",
        "\n",
        "# Print label-to-index mapping\n",
        "print(\"Label to index:\", dict(zip(le.classes_, le.transform(le.classes_))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJuFCqbvZcrR",
        "outputId": "1ba8f1c0-7015-431a-97cc-b4bdf991653f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label to index: {np.str_('F'): np.int64(0), np.str_('N'): np.int64(1), np.str_('Q'): np.int64(2), np.str_('S'): np.int64(3), np.str_('V'): np.int64(4)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TimeSeriesDataset(X, y_encoded)\n",
        "\n",
        "# ----- Split 85% / 15% -----\n",
        "train_size = int(0.85 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, pin_memory=True)"
      ],
      "metadata": {
        "id": "v5VHTflSZrF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "Counter(y_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4iQp2_AZvGs",
        "outputId": "f34f3e7e-ddb6-4d64-f9aa-e2a72969a91e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({np.int64(1): 90558,\n",
              "         np.int64(3): 2779,\n",
              "         np.int64(4): 7235,\n",
              "         np.int64(2): 9284,\n",
              "         np.int64(0): 802})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As seen from the class distribution, the dataset is highly imbalanced, To address this imbalance, we use a weighted cross-entropy loss, assigning higher importance to underrepresented classes. The weights are computed based on the inverse frequency of each class."
      ],
      "metadata": {
        "id": "MkN6cyeft_yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "model = CNNLSTMClassifier(num_classes=5)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "F1v_1KiDZzG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained for 20 epochs, and the learned weights are saved to Google Drive for later evaluation and inference."
      ],
      "metadata": {
        "id": "jLhwGvx2uk9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Training -----\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/model_weights.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCu4D0MsZ3Mp",
        "outputId": "e88dc385-a989-41a4-9648-48cf988abfac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 1.1212\n",
            "Epoch 2/20, Loss: 0.7310\n",
            "Epoch 3/20, Loss: 0.6211\n",
            "Epoch 4/20, Loss: 0.5104\n",
            "Epoch 5/20, Loss: 0.4640\n",
            "Epoch 6/20, Loss: 0.4092\n",
            "Epoch 7/20, Loss: 0.3925\n",
            "Epoch 8/20, Loss: 0.3861\n",
            "Epoch 9/20, Loss: 0.3462\n",
            "Epoch 10/20, Loss: 0.3410\n",
            "Epoch 11/20, Loss: 0.3586\n",
            "Epoch 12/20, Loss: 0.3140\n",
            "Epoch 13/20, Loss: 0.3096\n",
            "Epoch 14/20, Loss: 0.3127\n",
            "Epoch 15/20, Loss: 0.2982\n",
            "Epoch 16/20, Loss: 0.3249\n",
            "Epoch 17/20, Loss: 0.2818\n",
            "Epoch 18/20, Loss: 0.2926\n",
            "Epoch 19/20, Loss: 0.2626\n",
            "Epoch 20/20, Loss: 0.2698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/model_weights.pth'))  #load the weights from our drive\n",
        "model.eval()\n",
        "model.to('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "mrPxi52VaBFG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c1c59a8-7ae0-492f-e793-7717f860b5ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNNLSTMClassifier(\n",
              "  (conv1): Conv1d(1, 64, kernel_size=(50,), stride=(1,))\n",
              "  (pool1): MaxPool1d(kernel_size=20, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (dropout1): Dropout(p=0.1, inplace=False)\n",
              "  (conv2): Conv1d(64, 32, kernel_size=(10,), stride=(1,))\n",
              "  (pool2): MaxPool1d(kernel_size=10, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (dropout2): Dropout(p=0.1, inplace=False)\n",
              "  (conv3): Conv1d(32, 16, kernel_size=(5,), stride=(1,))\n",
              "  (pool3): MaxPool1d(kernel_size=5, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (dropout3): Dropout(p=0.1, inplace=False)\n",
              "  (lstm): LSTM(16, 32, batch_first=True)\n",
              "  (fc1): Linear(in_features=32, out_features=32, bias=True)\n",
              "  (dropout4): Dropout(p=0.1, inplace=False)\n",
              "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
              "  (fc3): Linear(in_features=16, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation\n",
        "The trained model is evaluated on the validation dataset to compute accuracy and other performance metrics, including F1 score, sensitivity, and specificity for each class"
      ],
      "metadata": {
        "id": "bcGHuRi0u4hM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs = inputs.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        labels = labels.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "y_pred = np.array(all_preds)\n",
        "y_true = np.array(all_labels)\n"
      ],
      "metadata": {
        "id": "oh4bEbXIpQd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
        "\n",
        "# Classification report for per-class metrics\n",
        "report = classification_report(y_true, y_pred, output_dict=True)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred))\n",
        "\n",
        "# Overall F1 score (macro/weighted)\n",
        "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "print(f\"\\nF1 Score: {f1_macro:.4f}\")\n",
        "\n",
        "# Sensitivity (Recall) and Specificity\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "TP = conf_matrix.diagonal()\n",
        "FN = conf_matrix.sum(axis=1) - TP\n",
        "FP = conf_matrix.sum(axis=0) - TP\n",
        "TN = conf_matrix.sum() - (TP + FP + FN)\n",
        "\n",
        "sensitivity = TP / (TP + FN)  # Recall per class\n",
        "specificity = TN / (TN + FP)\n",
        "\n",
        "print(\"\\nSensitivity per class:\", sensitivity)\n",
        "print(\"Specificity per class:\", specificity)\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f\"\\nValidation Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJwPz3bbpVMf",
        "outputId": "63c2099b-5019-44e0-9d76-3f237b14b623"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.20      0.87      0.33       119\n",
            "           1       0.99      0.91      0.95     13585\n",
            "           2       0.93      0.96      0.95      1385\n",
            "           3       0.30      0.81      0.44       403\n",
            "           4       0.87      0.86      0.86      1107\n",
            "\n",
            "    accuracy                           0.91     16599\n",
            "   macro avg       0.66      0.88      0.71     16599\n",
            "weighted avg       0.95      0.91      0.93     16599\n",
            "\n",
            "\n",
            "F1 Score: 0.7050\n",
            "\n",
            "Sensitivity per class: [0.86554622 0.91012146 0.95884477 0.81389578 0.85817525]\n",
            "Specificity per class: [0.97518204 0.95952223 0.99375575 0.95332181 0.99070488]\n",
            "\n",
            "Validation Accuracy: 0.9081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "The model achieves a validation accuracy of 90.81%, demonstrating strong performance in classifying arrhythmia types.\n",
        "\n",
        "Class Mapping:\n",
        "\n",
        "0 → F (Fusion)\n",
        "\n",
        "1 → N (Normal)\n",
        "\n",
        "2 → Q (Unknown)\n",
        "\n",
        "3 → S (Supraventricular ectopic)\n",
        "\n",
        "4 → V (Ventricular ectopic)\n",
        "\n",
        "Sensitivity per class:\n",
        "[0.8655 (F), 0.9101 (N), 0.9588 (Q), 0.8139 (S), 0.8582 (V)]\n",
        "\n",
        "Specificity per class:\n",
        "[0.9752 (F), 0.9595 (N), 0.9938 (Q), 0.9533 (S), 0.9907 (V)]\n",
        "\n",
        "These results indicate balanced and reliable detection performance across all AAMI classes."
      ],
      "metadata": {
        "id": "A73cpVJNvToY"
      }
    }
  ]
}